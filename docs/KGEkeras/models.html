<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>KGEkeras.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>KGEkeras.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">### KG embedding version 3.

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Embedding, Lambda, Multiply, Reshape, Concatenate, BatchNormalization, Conv2D, Activation, Dense, Dropout, Conv3D, Flatten, Dot
import tensorflow as tf
import tensorflow.keras.backend as K
import numpy as np


def l3_reg(weight_matrix, w = 0.01):
    return w * tf.norm(weight_matrix,ord=3)**3

class EmbeddingModel(tf.keras.Model):
    def __init__(self, 
                 e_dim, 
                 r_dim, 
                 num_entities, 
                 num_relations, 
                 negative_samples=2, 
                 batch_size=16,
                 loss_function = &#39;pointwize_hinge&#39;,
                 dp = 0.2,
                 margin = 1,
                 loss_weight=1,
                 regularization = 0.0,
                 literal_activation=None,
                 literals=None,
                 use_batch_norm=False,
                 entity_embedding_args = None,
                 relational_embedding_args = None,
                 init_entities = None, 
                 init_relations = None,
                 name=&#39;embedding_model&#39;,
                 **kwargs):
        &#34;&#34;&#34;
        Base class for embedding models. 
        
        Parameters
        ----------
        e_dim : int 
            Entity embedding dimension
            
        r_dim : int 
            Relation embedding dimension
        
        num_entities : int 
        
        num_relations : int
        
        negative_samples : int 
            Number of negative triples per BATCH.
            
        loss_function : string
            hinge, logistic, or square
        
        loss_type : string 
            pointwize or pairwize
        
        literal_activation : string or tf.keras.activation.Activation
            if using LiteralE methodology. 
            relu function
            sigmoid function
            softmax function
            softplus function
            softsign function
            tanh function
            selu function
            elu function
            exponential function
        
        use_bn : bool 
            Batch norm. 
            
        use_dp : bool 
            Use dropout.
        &#34;&#34;&#34;
        super(EmbeddingModel, self).__init__()
        self.regularization = regularization
        if regularization != 0.0:
            reg = lambda x: l3_reg(x,regularization)
        else:
            reg = None
        
        self.num_entities = num_entities
        self.num_relations = num_relations
        
        init_e = tf.keras.initializers.GlorotUniform()
        init_r = tf.keras.initializers.GlorotUniform()
        self.entity_embedding = Embedding(num_entities,
                                          e_dim,
                                          embeddings_initializer=init_e, 
                                          embeddings_regularizer=reg, 
                                          name=name+&#39;_entity_embedding&#39;)
        if init_entities is not None: 
            self.entity_embedding = Embedding(num_entities,
                                          e_dim,
                                          weights=[init_entities],
                                          embeddings_regularizer=reg, 
                                          name=name+&#39;_entity_embedding&#39;)
            
        self.relational_embedding = Embedding(num_relations,
                                              r_dim,
                                              embeddings_initializer=init_r, 
                                              #embeddings_regularizer=reg, 
                                              name=name+&#39;_relational_embedding&#39;)
        
        if init_relations is not None: 
            self.relational_embedding = Embedding(num_relations,
                                              r_dim,
                                              weights=[init_relations],
                                              #embeddings_regularizer=reg, 
                                              name=name+&#39;_relational_embedding&#39;)
        
        if literal_activation:
            self.literal_layer = Embedding(len(literals),
                                           len(literals[0]),
                                           weights=[literals],
                                           name=name+&#39;_literals&#39;)
            self.literal_layer.trainable=False
            self.literal_activation = Dense(e_dim,activation=literal_activation)
        
        self.dp = Dropout(dp)
        self.e_dim = e_dim
        self.r_dim = r_dim
        self.margin = margin
        self.loss_weight = loss_weight
        self.regularization = regularization
       
        self.__dict__.update(kwargs)
    
    def get_config(self):
        return self.__dict__
    
    def call(self,inputs,training=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        inputs : tensor, shape = (batch_size, 3)
        &#34;&#34;&#34;
        s,p,o = tf.unstack(inputs,axis=1)
        l_s = s
        l_o = o
        
        def lookup_entity(a):
            return self.dp(self.entity_embedding(a))
            
        def lookup_relation(a):
            return self.dp(self.relational_embedding(a))
            
        s,p,o = lookup_entity(s),lookup_relation(p),lookup_entity(o)
        
        if hasattr(self,&#39;literal_layer&#39;):
            l_s = self.dp(self.literal_layer(l_s))
            l_o = self.dp(self.literal_layer(l_o))
            s = tf.concat([s,l_s],axis=-1)
            o = tf.concat([o,l_o],axis=-1)
            s = self.literal_activation(s)
            o = self.literal_activation(o)
        
        score = self.func(s,p,o,training)
        
        return score
    
class DistMult(EmbeddingModel):
    def __init__(self,
                 name=&#39;DistMult&#39;, 
                 **kwargs):
        &#34;&#34;&#34;DistMult implmentation.&#34;&#34;&#34;
        super(DistMult, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        return tf.reduce_sum(s*p*o, axis=-1)
        

class TransE(EmbeddingModel):
    def __init__(self, 
                 name=&#39;TransE&#39;,
                 norm=1,
                 gamma=12,
                 **kwargs):
        &#34;&#34;&#34;TransE implmentation.&#34;&#34;&#34;
        super(TransE, self).__init__(name=name,**kwargs)
        self.gamma = gamma
        self.norm = norm
        
    def func(self, s,p,o, training = False):
        if self.gamma &gt; 0:
            return self.gamma - tf.norm(s+p-o, axis=1, ord=self.norm)
        else:
            return tf.norm(s+p-o, axis=1, ord=self.norm)
    
class CosinE(EmbeddingModel):
    def __init__(self,
                 name=&#39;CosinE&#39;,
                 **kwargs):
        &#34;&#34;&#34;CosinE implmentation.&#34;&#34;&#34;
        super(CosinE, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        return -(1+2*cosine_similarity(s+p,o,axis=1))
        

class ComplEx(EmbeddingModel):
    def __init__(self,
                 name=&#39;ComplEx&#39;, 
                 **kwargs):
        &#34;&#34;&#34;ComplEx implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 2*kwargs[&#39;r_dim&#39;]
        super(ComplEx, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
        s_real, s_img = split2(s)
        p_real, p_img = split2(p)
        o_real, o_img = split2(o)
        
        s1 = s_real*p_real*o_real
        s2 = p_real*s_img*o_img
        s3 = p_img*s_real*o_img
        s4 = p_img*s_img*o_real
        return tf.reduce_sum(s1+s2+s3-s4,axis=-1)

class HolE(EmbeddingModel):
    def __init__(self,
                 name=&#39;HolE&#39;, 
                 **kwargs):
        &#34;&#34;&#34;HolE implmentation.&#34;&#34;&#34;
        super(HolE, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        def circular_cross_correlation(a, b):
            return tf.math.real(tf.signal.ifft(
            tf.multiply(tf.math.conj(tf.signal.fft(tf.cast(a, tf.complex64))), tf.signal.fft(tf.cast(b, tf.complex64)))))
        
        x = circular_cross_correlation(s,o)
        return tf.reduce_sum(p*x,axis=-1)

class ConvE(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvE&#39;, 
                 hidden_dp=0.2,
                 conv_filters=8,
                 conv_size_w=3, 
                 conv_size_h=3,
                 **kwargs):
        &#34;&#34;&#34;ConvE implmentation.&#34;&#34;&#34;
        super(ConvE, self).__init__(name=name,**kwargs)
        self.dim = kwargs[&#39;e_dim&#39;]
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        self.w, self.h = factors(self.dim).pop(-1)
        assert self.w &gt; 1 or self.h &gt; 1

        self.ls = [Conv2D(conv_filters,(conv_size_w,conv_size_h)),
                   BatchNormalization(),
                    Activation(&#39;relu&#39;),
                    Dropout(hidden_dp),
                    Flatten(),
                    Dense(self.dim),
                    BatchNormalization(),
                    Activation(&#39;relu&#39;),
                    Dropout(hidden_dp)]
        
    def func(self, s,p,o, training = False):
        s = tf.reshape(s,(-1,self.w,self.h,1))
        p = tf.reshape(p,(-1,self.w,self.h,1))
        x = tf.concat([s,p],axis=1)
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
            
        return tf.reduce_sum(x * o, axis=-1)
    
class ConvR(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvR&#39;, 
                 hidden_dp=0.2,
                 conv_filters=8,
                 conv_size_w=3, 
                 conv_size_h=3,
                 **kwargs):
        &#34;&#34;&#34;ConvR implmentation.&#34;&#34;&#34;
        kwargs[&#39;r_dim&#39;] = conv_filters*conv_size_w*conv_size_h
        super(ConvR, self).__init__(name=name,**kwargs)
        
        self.dim = kwargs[&#39;e_dim&#39;]
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        self.w, self.h = factors(self.dim).pop(-1)
       
        self.conv_filters = conv_filters
        self.conv_size_h = conv_size_h
        self.conv_size_w = conv_size_w
        
        self.ls = [
            Flatten(),
            Activation(&#39;relu&#39;),
            Dense(self.dim),
            Dropout(hidden_dp),
            Activation(&#39;relu&#39;)
            ]
        
    def func(self,s,p,o, training = False):
        
        def forward(x):
            a, b = x[:self.e_dim], x[self.e_dim:]
            a = tf.reshape(a, (1,self.w,self.h,1))
            b = tf.reshape(b,(self.conv_size_w,self.conv_size_h,1,self.conv_filters))
            return tf.nn.conv2d(a, b, strides = [1,1], padding=&#39;SAME&#39;)
        
        x = tf.map_fn(forward, tf.concat([s,p],axis=-1))
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
     
        return tf.reduce_sum(x * o, axis=-1)
    
class ConvKB(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvKB&#39;, 
                 hidden_dp=0.2,
                 conv_filters=3,
                 num_blocks = 1,
                 **kwargs):
        &#34;&#34;&#34;ConvKB implmentation.&#34;&#34;&#34;
        super(ConvKB, self).__init__(name=name,**kwargs)
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        
        self.dim = kwargs[&#39;e_dim&#39;]
        
        self.w, self.h = self.dim, 3
        
        block = [Conv2D(conv_filters,(1,3),strides=(1,1)),
                 BatchNormalization(),
                  Activation(&#39;relu&#39;),
                  Dropout(hidden_dp)]
        
        self.ls = []
        for _ in range(num_blocks):
            self.ls.extend(block)
        
        self.ls.extend([Reshape((3,-1)),
                        Lambda(lambda x: tf.reduce_sum(x[:,0]*x[:,1]*x[:,2],axis=-1))])
        
    def func(self, s,p,o, training = False):
        x = tf.concat([s,p,o],axis=-1)
        x = tf.reshape(x,(-1,self.w,self.h,1))
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
        
        return x

class HAKE(EmbeddingModel):
    def __init__(self,
                 epsilon=2, 
                 gamma = 12,  
                 phase_weight = 0.5,
                 mod_weight = 1,
                 name=&#39;HAKE&#39;,
                 **kwargs):
        &#34;&#34;&#34;HAKE implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 3*kwargs[&#39;r_dim&#39;]
        super(HAKE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        self.phase_weight = phase_weight
        self.mod_weight = mod_weight
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        
    def func(self, s,p,o, training = False):
        split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
        split3 = lambda x: tf.split(x,num_or_size_splits=3,axis=-1)
        
        phase_s, mod_s = split2(s)
        phase_o, mod_o = split2(o)
        phase_p, mod_p, bias_p = split3(p)
        
        phase_s = phase_s / (self.embedding_range / self.pi)
        phase_p = phase_p / (self.embedding_range / self.pi)
        phase_o = phase_o / (self.embedding_range / self.pi)
        
        bias_p = K.clip(bias_p,min_value=-np.Inf,max_value=1.)
        bias_p = tf.where(bias_p &lt; -K.abs(mod_p), -K.abs(mod_p), bias_p)
        
        r_score = self.mod_weight*tf.norm(mod_s * (mod_p + bias_p) - K.abs(mod_o) * (1-bias_p), ord=2)
        p_score = self.phase_weight*tf.norm(tf.math.sin((phase_s+phase_p-phase_o)/2), ord=1,axis=-1)
        return self.gamma - (p_score + r_score)
        
        
class ModE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 norm = 2,
                 name=&#39;ModE&#39;, 
                 **kwargs):
        &#34;&#34;&#34;ModE implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 3*kwargs[&#39;r_dim&#39;]
        super(ModE, self).__init__(name=name,**kwargs)
        
        self.norm = norm
        self.gamma
       
    def func(self, s,p,o, training = False):
        return self.gamma - tf.norm(s * p - o, ord=self.norm, axis=-1)
        
    
class RotatE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 norm=2,
                 epsilon=2,
                 name=&#39;RotatE&#39;,
                 **kwargs):
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = kwargs[&#39;r_dim&#39;]
        super(RotatE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        self.norm = norm
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        
    def func(self, s,p,o, training=False):
        re_s, im_s = tf.split(s,num_or_size_splits=2,axis=-1)
        re_o, im_o = tf.split(o,num_or_size_splits=2,axis=-1)
        
        phase_r = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
        
        re_r = tf.math.cos(phase_r)
        im_r = tf.math.sin(phase_r)
        
        re_score = re_s * re_r - im_s * im_r
        im_score = re_s * im_r + im_s * re_r
        re_score = re_score - re_o
        im_score = im_score - im_o
        
        score = tf.concat([re_score,im_score],axis=1)
        score = tf.reduce_sum(score,axis=1)
        
        if self.gamma &gt; 0:
            return self.gamma - score
        else:
            return score
    
class pRotatE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 epsilon=2,
                 modulus=0.5,
                 name=&#39;pRotatE&#39;,
                 **kwargs):
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 2*kwargs[&#39;r_dim&#39;]
        super(pRotatE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        self.modulus = modulus*self.embedding_range
        
    def func(self, s,p,o, training=False):
        
        phase_s = tf.math.atan2(tf.math.sin(s),tf.math.cos(s))
        phase_p = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
        phase_o = tf.math.atan2(tf.math.sin(o),tf.math.cos(o))
        
        score = tf.abs(tf.math.sin((phase_s + phase_p - phase_o)/2))
        if self.gamma &gt; 0:
            return self.gamma - tf.reduce_sum(score,axis=1)*self.modulus
        else:
            return tf.reduce_sum(score,axis=1)*self.modulus
        
        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="KGEkeras.models.l3_reg"><code class="name flex">
<span>def <span class="ident">l3_reg</span></span>(<span>weight_matrix, w=0.01)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l3_reg(weight_matrix, w = 0.01):
    return w * tf.norm(weight_matrix,ord=3)**3</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="KGEkeras.models.ComplEx"><code class="flex name class">
<span>class <span class="ident">ComplEx</span></span>
<span>(</span><span>name='ComplEx', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>ComplEx implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ComplEx(EmbeddingModel):
    def __init__(self,
                 name=&#39;ComplEx&#39;, 
                 **kwargs):
        &#34;&#34;&#34;ComplEx implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 2*kwargs[&#39;r_dim&#39;]
        super(ComplEx, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
        s_real, s_img = split2(s)
        p_real, p_img = split2(p)
        o_real, o_img = split2(o)
        
        s1 = s_real*p_real*o_real
        s2 = p_real*s_img*o_img
        s3 = p_img*s_real*o_img
        s4 = p_img*s_img*o_real
        return tf.reduce_sum(s1+s2+s3-s4,axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.ComplEx.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
    s_real, s_img = split2(s)
    p_real, p_img = split2(p)
    o_real, o_img = split2(o)
    
    s1 = s_real*p_real*o_real
    s2 = p_real*s_img*o_img
    s3 = p_img*s_real*o_img
    s4 = p_img*s_img*o_real
    return tf.reduce_sum(s1+s2+s3-s4,axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.ConvE"><code class="flex name class">
<span>class <span class="ident">ConvE</span></span>
<span>(</span><span>name='ConvE', hidden_dp=0.2, conv_filters=8, conv_size_w=3, conv_size_h=3, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>ConvE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvE(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvE&#39;, 
                 hidden_dp=0.2,
                 conv_filters=8,
                 conv_size_w=3, 
                 conv_size_h=3,
                 **kwargs):
        &#34;&#34;&#34;ConvE implmentation.&#34;&#34;&#34;
        super(ConvE, self).__init__(name=name,**kwargs)
        self.dim = kwargs[&#39;e_dim&#39;]
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        self.w, self.h = factors(self.dim).pop(-1)
        assert self.w &gt; 1 or self.h &gt; 1

        self.ls = [Conv2D(conv_filters,(conv_size_w,conv_size_h)),
                   BatchNormalization(),
                    Activation(&#39;relu&#39;),
                    Dropout(hidden_dp),
                    Flatten(),
                    Dense(self.dim),
                    BatchNormalization(),
                    Activation(&#39;relu&#39;),
                    Dropout(hidden_dp)]
        
    def func(self, s,p,o, training = False):
        s = tf.reshape(s,(-1,self.w,self.h,1))
        p = tf.reshape(p,(-1,self.w,self.h,1))
        x = tf.concat([s,p],axis=1)
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
            
        return tf.reduce_sum(x * o, axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.ConvE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    s = tf.reshape(s,(-1,self.w,self.h,1))
    p = tf.reshape(p,(-1,self.w,self.h,1))
    x = tf.concat([s,p],axis=1)
    
    for l in self.ls:
        if isinstance(l,(Dropout, BatchNormalization)):
            x = l(x,training=training)
        else:
            x = l(x)
        
    return tf.reduce_sum(x * o, axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.ConvKB"><code class="flex name class">
<span>class <span class="ident">ConvKB</span></span>
<span>(</span><span>name='ConvKB', hidden_dp=0.2, conv_filters=3, num_blocks=1, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>ConvKB implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvKB(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvKB&#39;, 
                 hidden_dp=0.2,
                 conv_filters=3,
                 num_blocks = 1,
                 **kwargs):
        &#34;&#34;&#34;ConvKB implmentation.&#34;&#34;&#34;
        super(ConvKB, self).__init__(name=name,**kwargs)
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        
        self.dim = kwargs[&#39;e_dim&#39;]
        
        self.w, self.h = self.dim, 3
        
        block = [Conv2D(conv_filters,(1,3),strides=(1,1)),
                 BatchNormalization(),
                  Activation(&#39;relu&#39;),
                  Dropout(hidden_dp)]
        
        self.ls = []
        for _ in range(num_blocks):
            self.ls.extend(block)
        
        self.ls.extend([Reshape((3,-1)),
                        Lambda(lambda x: tf.reduce_sum(x[:,0]*x[:,1]*x[:,2],axis=-1))])
        
    def func(self, s,p,o, training = False):
        x = tf.concat([s,p,o],axis=-1)
        x = tf.reshape(x,(-1,self.w,self.h,1))
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
        
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.ConvKB.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    x = tf.concat([s,p,o],axis=-1)
    x = tf.reshape(x,(-1,self.w,self.h,1))
    
    for l in self.ls:
        if isinstance(l,(Dropout, BatchNormalization)):
            x = l(x,training=training)
        else:
            x = l(x)
    
    return x</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.ConvR"><code class="flex name class">
<span>class <span class="ident">ConvR</span></span>
<span>(</span><span>name='ConvR', hidden_dp=0.2, conv_filters=8, conv_size_w=3, conv_size_h=3, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>ConvR implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvR(EmbeddingModel):
    def __init__(self,
                 name=&#39;ConvR&#39;, 
                 hidden_dp=0.2,
                 conv_filters=8,
                 conv_size_w=3, 
                 conv_size_h=3,
                 **kwargs):
        &#34;&#34;&#34;ConvR implmentation.&#34;&#34;&#34;
        kwargs[&#39;r_dim&#39;] = conv_filters*conv_size_w*conv_size_h
        super(ConvR, self).__init__(name=name,**kwargs)
        
        self.dim = kwargs[&#39;e_dim&#39;]
        factors = lambda val: [(int(i), int(val / i)) for i in range(1, int(val**0.5)+1) if val % i == 0]
        self.w, self.h = factors(self.dim).pop(-1)
       
        self.conv_filters = conv_filters
        self.conv_size_h = conv_size_h
        self.conv_size_w = conv_size_w
        
        self.ls = [
            Flatten(),
            Activation(&#39;relu&#39;),
            Dense(self.dim),
            Dropout(hidden_dp),
            Activation(&#39;relu&#39;)
            ]
        
    def func(self,s,p,o, training = False):
        
        def forward(x):
            a, b = x[:self.e_dim], x[self.e_dim:]
            a = tf.reshape(a, (1,self.w,self.h,1))
            b = tf.reshape(b,(self.conv_size_w,self.conv_size_h,1,self.conv_filters))
            return tf.nn.conv2d(a, b, strides = [1,1], padding=&#39;SAME&#39;)
        
        x = tf.map_fn(forward, tf.concat([s,p],axis=-1))
        
        for l in self.ls:
            if isinstance(l,(Dropout, BatchNormalization)):
                x = l(x,training=training)
            else:
                x = l(x)
     
        return tf.reduce_sum(x * o, axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.ConvR.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,s,p,o, training = False):
    
    def forward(x):
        a, b = x[:self.e_dim], x[self.e_dim:]
        a = tf.reshape(a, (1,self.w,self.h,1))
        b = tf.reshape(b,(self.conv_size_w,self.conv_size_h,1,self.conv_filters))
        return tf.nn.conv2d(a, b, strides = [1,1], padding=&#39;SAME&#39;)
    
    x = tf.map_fn(forward, tf.concat([s,p],axis=-1))
    
    for l in self.ls:
        if isinstance(l,(Dropout, BatchNormalization)):
            x = l(x,training=training)
        else:
            x = l(x)
 
    return tf.reduce_sum(x * o, axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.CosinE"><code class="flex name class">
<span>class <span class="ident">CosinE</span></span>
<span>(</span><span>name='CosinE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>CosinE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CosinE(EmbeddingModel):
    def __init__(self,
                 name=&#39;CosinE&#39;,
                 **kwargs):
        &#34;&#34;&#34;CosinE implmentation.&#34;&#34;&#34;
        super(CosinE, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        return -(1+2*cosine_similarity(s+p,o,axis=1))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.CosinE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    return -(1+2*cosine_similarity(s+p,o,axis=1))</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.DistMult"><code class="flex name class">
<span>class <span class="ident">DistMult</span></span>
<span>(</span><span>name='DistMult', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>DistMult implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DistMult(EmbeddingModel):
    def __init__(self,
                 name=&#39;DistMult&#39;, 
                 **kwargs):
        &#34;&#34;&#34;DistMult implmentation.&#34;&#34;&#34;
        super(DistMult, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        return tf.reduce_sum(s*p*o, axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.DistMult.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    return tf.reduce_sum(s*p*o, axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.EmbeddingModel"><code class="flex name class">
<span>class <span class="ident">EmbeddingModel</span></span>
<span>(</span><span>e_dim, r_dim, num_entities, num_relations, negative_samples=2, batch_size=16, loss_function='pointwize_hinge', dp=0.2, margin=1, loss_weight=1, regularization=0.0, literal_activation=None, literals=None, use_batch_norm=False, entity_embedding_args=None, relational_embedding_args=None, init_entities=None, init_relations=None, name='embedding_model', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>Base class for embedding models. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>e_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Entity embedding dimension</dd>
<dt><strong><code>r_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Relation embedding dimension</dd>
<dt><strong><code>num_entities</code></strong> :&ensp;<code>int </code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>num_relations</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>negative_samples</code></strong> :&ensp;<code>int </code></dt>
<dd>Number of negative triples per BATCH.</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>string</code></dt>
<dd>hinge, logistic, or square</dd>
<dt><strong><code>loss_type</code></strong> :&ensp;<code>string </code></dt>
<dd>pointwize or pairwize</dd>
<dt><strong><code>literal_activation</code></strong> :&ensp;<code>string</code> or <code>tf.keras.activation.Activation</code></dt>
<dd>if using LiteralE methodology.
relu function
sigmoid function
softmax function
softplus function
softsign function
tanh function
selu function
elu function
exponential function</dd>
<dt><strong><code>use_bn</code></strong> :&ensp;<code>bool </code></dt>
<dd>Batch norm.</dd>
<dt><strong><code>use_dp</code></strong> :&ensp;<code>bool </code></dt>
<dd>Use dropout.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmbeddingModel(tf.keras.Model):
    def __init__(self, 
                 e_dim, 
                 r_dim, 
                 num_entities, 
                 num_relations, 
                 negative_samples=2, 
                 batch_size=16,
                 loss_function = &#39;pointwize_hinge&#39;,
                 dp = 0.2,
                 margin = 1,
                 loss_weight=1,
                 regularization = 0.0,
                 literal_activation=None,
                 literals=None,
                 use_batch_norm=False,
                 entity_embedding_args = None,
                 relational_embedding_args = None,
                 init_entities = None, 
                 init_relations = None,
                 name=&#39;embedding_model&#39;,
                 **kwargs):
        &#34;&#34;&#34;
        Base class for embedding models. 
        
        Parameters
        ----------
        e_dim : int 
            Entity embedding dimension
            
        r_dim : int 
            Relation embedding dimension
        
        num_entities : int 
        
        num_relations : int
        
        negative_samples : int 
            Number of negative triples per BATCH.
            
        loss_function : string
            hinge, logistic, or square
        
        loss_type : string 
            pointwize or pairwize
        
        literal_activation : string or tf.keras.activation.Activation
            if using LiteralE methodology. 
            relu function
            sigmoid function
            softmax function
            softplus function
            softsign function
            tanh function
            selu function
            elu function
            exponential function
        
        use_bn : bool 
            Batch norm. 
            
        use_dp : bool 
            Use dropout.
        &#34;&#34;&#34;
        super(EmbeddingModel, self).__init__()
        self.regularization = regularization
        if regularization != 0.0:
            reg = lambda x: l3_reg(x,regularization)
        else:
            reg = None
        
        self.num_entities = num_entities
        self.num_relations = num_relations
        
        init_e = tf.keras.initializers.GlorotUniform()
        init_r = tf.keras.initializers.GlorotUniform()
        self.entity_embedding = Embedding(num_entities,
                                          e_dim,
                                          embeddings_initializer=init_e, 
                                          embeddings_regularizer=reg, 
                                          name=name+&#39;_entity_embedding&#39;)
        if init_entities is not None: 
            self.entity_embedding = Embedding(num_entities,
                                          e_dim,
                                          weights=[init_entities],
                                          embeddings_regularizer=reg, 
                                          name=name+&#39;_entity_embedding&#39;)
            
        self.relational_embedding = Embedding(num_relations,
                                              r_dim,
                                              embeddings_initializer=init_r, 
                                              #embeddings_regularizer=reg, 
                                              name=name+&#39;_relational_embedding&#39;)
        
        if init_relations is not None: 
            self.relational_embedding = Embedding(num_relations,
                                              r_dim,
                                              weights=[init_relations],
                                              #embeddings_regularizer=reg, 
                                              name=name+&#39;_relational_embedding&#39;)
        
        if literal_activation:
            self.literal_layer = Embedding(len(literals),
                                           len(literals[0]),
                                           weights=[literals],
                                           name=name+&#39;_literals&#39;)
            self.literal_layer.trainable=False
            self.literal_activation = Dense(e_dim,activation=literal_activation)
        
        self.dp = Dropout(dp)
        self.e_dim = e_dim
        self.r_dim = r_dim
        self.margin = margin
        self.loss_weight = loss_weight
        self.regularization = regularization
       
        self.__dict__.update(kwargs)
    
    def get_config(self):
        return self.__dict__
    
    def call(self,inputs,training=False):
        &#34;&#34;&#34;
        Parameters
        ----------
        inputs : tensor, shape = (batch_size, 3)
        &#34;&#34;&#34;
        s,p,o = tf.unstack(inputs,axis=1)
        l_s = s
        l_o = o
        
        def lookup_entity(a):
            return self.dp(self.entity_embedding(a))
            
        def lookup_relation(a):
            return self.dp(self.relational_embedding(a))
            
        s,p,o = lookup_entity(s),lookup_relation(p),lookup_entity(o)
        
        if hasattr(self,&#39;literal_layer&#39;):
            l_s = self.dp(self.literal_layer(l_s))
            l_o = self.dp(self.literal_layer(l_o))
            s = tf.concat([s,l_s],axis=-1)
            o = tf.concat([o,l_o],axis=-1)
            s = self.literal_activation(s)
            o = self.literal_activation(o)
        
        score = self.func(s,p,o,training)
        
        return score</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.ComplEx" href="#KGEkeras.models.ComplEx">ComplEx</a></li>
<li><a title="KGEkeras.models.ConvE" href="#KGEkeras.models.ConvE">ConvE</a></li>
<li><a title="KGEkeras.models.ConvKB" href="#KGEkeras.models.ConvKB">ConvKB</a></li>
<li><a title="KGEkeras.models.ConvR" href="#KGEkeras.models.ConvR">ConvR</a></li>
<li><a title="KGEkeras.models.CosinE" href="#KGEkeras.models.CosinE">CosinE</a></li>
<li><a title="KGEkeras.models.DistMult" href="#KGEkeras.models.DistMult">DistMult</a></li>
<li><a title="KGEkeras.models.HAKE" href="#KGEkeras.models.HAKE">HAKE</a></li>
<li><a title="KGEkeras.models.HolE" href="#KGEkeras.models.HolE">HolE</a></li>
<li><a title="KGEkeras.models.ModE" href="#KGEkeras.models.ModE">ModE</a></li>
<li><a title="KGEkeras.models.RotatE" href="#KGEkeras.models.RotatE">RotatE</a></li>
<li><a title="KGEkeras.models.TransE" href="#KGEkeras.models.TransE">TransE</a></li>
<li><a title="KGEkeras.models.pRotatE" href="#KGEkeras.models.pRotatE">pRotatE</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.EmbeddingModel.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, training=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tensor, shape = (batch_size, 3)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self,inputs,training=False):
    &#34;&#34;&#34;
    Parameters
    ----------
    inputs : tensor, shape = (batch_size, 3)
    &#34;&#34;&#34;
    s,p,o = tf.unstack(inputs,axis=1)
    l_s = s
    l_o = o
    
    def lookup_entity(a):
        return self.dp(self.entity_embedding(a))
        
    def lookup_relation(a):
        return self.dp(self.relational_embedding(a))
        
    s,p,o = lookup_entity(s),lookup_relation(p),lookup_entity(o)
    
    if hasattr(self,&#39;literal_layer&#39;):
        l_s = self.dp(self.literal_layer(l_s))
        l_o = self.dp(self.literal_layer(l_o))
        s = tf.concat([s,l_s],axis=-1)
        o = tf.concat([o,l_o],axis=-1)
        s = self.literal_activation(s)
        o = self.literal_activation(o)
    
    score = self.func(s,p,o,training)
    
    return score</code></pre>
</details>
</dd>
<dt id="KGEkeras.models.EmbeddingModel.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <code>Network</code> (one layer of abstraction above).</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    return self.__dict__</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="KGEkeras.models.HAKE"><code class="flex name class">
<span>class <span class="ident">HAKE</span></span>
<span>(</span><span>epsilon=2, gamma=12, phase_weight=0.5, mod_weight=1, name='HAKE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>HAKE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HAKE(EmbeddingModel):
    def __init__(self,
                 epsilon=2, 
                 gamma = 12,  
                 phase_weight = 0.5,
                 mod_weight = 1,
                 name=&#39;HAKE&#39;,
                 **kwargs):
        &#34;&#34;&#34;HAKE implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 3*kwargs[&#39;r_dim&#39;]
        super(HAKE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        self.phase_weight = phase_weight
        self.mod_weight = mod_weight
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        
    def func(self, s,p,o, training = False):
        split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
        split3 = lambda x: tf.split(x,num_or_size_splits=3,axis=-1)
        
        phase_s, mod_s = split2(s)
        phase_o, mod_o = split2(o)
        phase_p, mod_p, bias_p = split3(p)
        
        phase_s = phase_s / (self.embedding_range / self.pi)
        phase_p = phase_p / (self.embedding_range / self.pi)
        phase_o = phase_o / (self.embedding_range / self.pi)
        
        bias_p = K.clip(bias_p,min_value=-np.Inf,max_value=1.)
        bias_p = tf.where(bias_p &lt; -K.abs(mod_p), -K.abs(mod_p), bias_p)
        
        r_score = self.mod_weight*tf.norm(mod_s * (mod_p + bias_p) - K.abs(mod_o) * (1-bias_p), ord=2)
        p_score = self.phase_weight*tf.norm(tf.math.sin((phase_s+phase_p-phase_o)/2), ord=1,axis=-1)
        return self.gamma - (p_score + r_score)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.HAKE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    split2 = lambda x: tf.split(x,num_or_size_splits=2,axis=-1)
    split3 = lambda x: tf.split(x,num_or_size_splits=3,axis=-1)
    
    phase_s, mod_s = split2(s)
    phase_o, mod_o = split2(o)
    phase_p, mod_p, bias_p = split3(p)
    
    phase_s = phase_s / (self.embedding_range / self.pi)
    phase_p = phase_p / (self.embedding_range / self.pi)
    phase_o = phase_o / (self.embedding_range / self.pi)
    
    bias_p = K.clip(bias_p,min_value=-np.Inf,max_value=1.)
    bias_p = tf.where(bias_p &lt; -K.abs(mod_p), -K.abs(mod_p), bias_p)
    
    r_score = self.mod_weight*tf.norm(mod_s * (mod_p + bias_p) - K.abs(mod_o) * (1-bias_p), ord=2)
    p_score = self.phase_weight*tf.norm(tf.math.sin((phase_s+phase_p-phase_o)/2), ord=1,axis=-1)
    return self.gamma - (p_score + r_score)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.HolE"><code class="flex name class">
<span>class <span class="ident">HolE</span></span>
<span>(</span><span>name='HolE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>HolE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HolE(EmbeddingModel):
    def __init__(self,
                 name=&#39;HolE&#39;, 
                 **kwargs):
        &#34;&#34;&#34;HolE implmentation.&#34;&#34;&#34;
        super(HolE, self).__init__(name=name,**kwargs)
        
    def func(self, s,p,o, training = False):
        def circular_cross_correlation(a, b):
            return tf.math.real(tf.signal.ifft(
            tf.multiply(tf.math.conj(tf.signal.fft(tf.cast(a, tf.complex64))), tf.signal.fft(tf.cast(b, tf.complex64)))))
        
        x = circular_cross_correlation(s,o)
        return tf.reduce_sum(p*x,axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.HolE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    def circular_cross_correlation(a, b):
        return tf.math.real(tf.signal.ifft(
        tf.multiply(tf.math.conj(tf.signal.fft(tf.cast(a, tf.complex64))), tf.signal.fft(tf.cast(b, tf.complex64)))))
    
    x = circular_cross_correlation(s,o)
    return tf.reduce_sum(p*x,axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.ModE"><code class="flex name class">
<span>class <span class="ident">ModE</span></span>
<span>(</span><span>gamma=12, norm=2, name='ModE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>ModE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 norm = 2,
                 name=&#39;ModE&#39;, 
                 **kwargs):
        &#34;&#34;&#34;ModE implmentation.&#34;&#34;&#34;
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 3*kwargs[&#39;r_dim&#39;]
        super(ModE, self).__init__(name=name,**kwargs)
        
        self.norm = norm
        self.gamma
       
    def func(self, s,p,o, training = False):
        return self.gamma - tf.norm(s * p - o, ord=self.norm, axis=-1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.ModE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    return self.gamma - tf.norm(s * p - o, ord=self.norm, axis=-1)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.RotatE"><code class="flex name class">
<span>class <span class="ident">RotatE</span></span>
<span>(</span><span>gamma=12, norm=2, epsilon=2, name='RotatE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>Base class for embedding models. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>e_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Entity embedding dimension</dd>
<dt><strong><code>r_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Relation embedding dimension</dd>
<dt><strong><code>num_entities</code></strong> :&ensp;<code>int </code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>num_relations</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>negative_samples</code></strong> :&ensp;<code>int </code></dt>
<dd>Number of negative triples per BATCH.</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>string</code></dt>
<dd>hinge, logistic, or square</dd>
<dt><strong><code>loss_type</code></strong> :&ensp;<code>string </code></dt>
<dd>pointwize or pairwize</dd>
<dt><strong><code>literal_activation</code></strong> :&ensp;<code>string</code> or <code>tf.keras.activation.Activation</code></dt>
<dd>if using LiteralE methodology.
relu function
sigmoid function
softmax function
softplus function
softsign function
tanh function
selu function
elu function
exponential function</dd>
<dt><strong><code>use_bn</code></strong> :&ensp;<code>bool </code></dt>
<dd>Batch norm.</dd>
<dt><strong><code>use_dp</code></strong> :&ensp;<code>bool </code></dt>
<dd>Use dropout.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RotatE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 norm=2,
                 epsilon=2,
                 name=&#39;RotatE&#39;,
                 **kwargs):
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = kwargs[&#39;r_dim&#39;]
        super(RotatE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        self.norm = norm
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        
    def func(self, s,p,o, training=False):
        re_s, im_s = tf.split(s,num_or_size_splits=2,axis=-1)
        re_o, im_o = tf.split(o,num_or_size_splits=2,axis=-1)
        
        phase_r = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
        
        re_r = tf.math.cos(phase_r)
        im_r = tf.math.sin(phase_r)
        
        re_score = re_s * re_r - im_s * im_r
        im_score = re_s * im_r + im_s * re_r
        re_score = re_score - re_o
        im_score = im_score - im_o
        
        score = tf.concat([re_score,im_score],axis=1)
        score = tf.reduce_sum(score,axis=1)
        
        if self.gamma &gt; 0:
            return self.gamma - score
        else:
            return score</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.RotatE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training=False):
    re_s, im_s = tf.split(s,num_or_size_splits=2,axis=-1)
    re_o, im_o = tf.split(o,num_or_size_splits=2,axis=-1)
    
    phase_r = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
    
    re_r = tf.math.cos(phase_r)
    im_r = tf.math.sin(phase_r)
    
    re_score = re_s * re_r - im_s * im_r
    im_score = re_s * im_r + im_s * re_r
    re_score = re_score - re_o
    im_score = im_score - im_o
    
    score = tf.concat([re_score,im_score],axis=1)
    score = tf.reduce_sum(score,axis=1)
    
    if self.gamma &gt; 0:
        return self.gamma - score
    else:
        return score</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.TransE"><code class="flex name class">
<span>class <span class="ident">TransE</span></span>
<span>(</span><span>name='TransE', norm=1, gamma=12, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>TransE implmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransE(EmbeddingModel):
    def __init__(self, 
                 name=&#39;TransE&#39;,
                 norm=1,
                 gamma=12,
                 **kwargs):
        &#34;&#34;&#34;TransE implmentation.&#34;&#34;&#34;
        super(TransE, self).__init__(name=name,**kwargs)
        self.gamma = gamma
        self.norm = norm
        
    def func(self, s,p,o, training = False):
        if self.gamma &gt; 0:
            return self.gamma - tf.norm(s+p-o, axis=1, ord=self.norm)
        else:
            return tf.norm(s+p-o, axis=1, ord=self.norm)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.TransE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training = False):
    if self.gamma &gt; 0:
        return self.gamma - tf.norm(s+p-o, axis=1, ord=self.norm)
    else:
        return tf.norm(s+p-o, axis=1, ord=self.norm)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="KGEkeras.models.pRotatE"><code class="flex name class">
<span>class <span class="ident">pRotatE</span></span>
<span>(</span><span>gamma=12, epsilon=2, modulus=0.5, name='pRotatE', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><code>Model</code> groups layers into an object with training and inference features.</p>
<h2 id="arguments">Arguments</h2>
<p>inputs: The input(s) of the model: a <code>keras.Input</code> object or list of
<code>keras.Input</code> objects.
outputs: The output(s) of the model. See Functional API example below.
name: String, the name of the model.</p>
<p>There are two ways to instantiate a <code>Model</code>:</p>
<p>1 - With the "Functional API", where you start from <code>Input</code>,
you chain layer calls to specify the model's forward pass,
and finally you create your model from inputs and outputs:</p>
<pre><code class="python">import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>
<p>2 - By subclassing the <code>Model</code> class: in that case, you should define your
layers in <code>__init__</code> and you should implement the model's forward pass
in <code>call</code>.</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)

  def call(self, inputs):
    x = self.dense1(inputs)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>If you subclass <code>Model</code>, you can optionally have
a <code>training</code> argument (boolean) in <code>call</code>, which you can use to specify
a different behavior in training and inference:</p>
<pre><code class="python">import tensorflow as tf

class MyModel(tf.keras.Model):

  def __init__(self):
    super(MyModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
    self.dropout = tf.keras.layers.Dropout(0.5)

  def call(self, inputs, training=False):
    x = self.dense1(inputs)
    if training:
      x = self.dropout(x, training=training)
    return self.dense2(x)

model = MyModel()
</code></pre>
<p>Once the model is created, you can config the model with losses and metrics
with <code>model.compile()</code>, train the model with <code>model.fit()</code>, or use the model
to do prediction with <code>model.predict()</code>.</p>
<p>Base class for embedding models. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>e_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Entity embedding dimension</dd>
<dt><strong><code>r_dim</code></strong> :&ensp;<code>int </code></dt>
<dd>Relation embedding dimension</dd>
<dt><strong><code>num_entities</code></strong> :&ensp;<code>int </code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>num_relations</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>negative_samples</code></strong> :&ensp;<code>int </code></dt>
<dd>Number of negative triples per BATCH.</dd>
<dt><strong><code>loss_function</code></strong> :&ensp;<code>string</code></dt>
<dd>hinge, logistic, or square</dd>
<dt><strong><code>loss_type</code></strong> :&ensp;<code>string </code></dt>
<dd>pointwize or pairwize</dd>
<dt><strong><code>literal_activation</code></strong> :&ensp;<code>string</code> or <code>tf.keras.activation.Activation</code></dt>
<dd>if using LiteralE methodology.
relu function
sigmoid function
softmax function
softplus function
softsign function
tanh function
selu function
elu function
exponential function</dd>
<dt><strong><code>use_bn</code></strong> :&ensp;<code>bool </code></dt>
<dd>Batch norm.</dd>
<dt><strong><code>use_dp</code></strong> :&ensp;<code>bool </code></dt>
<dd>Use dropout.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class pRotatE(EmbeddingModel):
    def __init__(self,
                 gamma=12,
                 epsilon=2,
                 modulus=0.5,
                 name=&#39;pRotatE&#39;,
                 **kwargs):
        kwargs[&#39;e_dim&#39;] = 2*kwargs[&#39;e_dim&#39;]
        kwargs[&#39;r_dim&#39;] = 2*kwargs[&#39;r_dim&#39;]
        super(pRotatE, self).__init__(name=name,**kwargs)
        
        self.gamma = gamma
        self.epsilon = epsilon
        
        self.pi = np.pi
        self.embedding_range = (self.gamma + self.epsilon) / self.e_dim / 2
        self.modulus = modulus*self.embedding_range
        
    def func(self, s,p,o, training=False):
        
        phase_s = tf.math.atan2(tf.math.sin(s),tf.math.cos(s))
        phase_p = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
        phase_o = tf.math.atan2(tf.math.sin(o),tf.math.cos(o))
        
        score = tf.abs(tf.math.sin((phase_s + phase_p - phase_o)/2))
        if self.gamma &gt; 0:
            return self.gamma - tf.reduce_sum(score,axis=1)*self.modulus
        else:
            return tf.reduce_sum(score,axis=1)*self.modulus</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="KGEkeras.models.pRotatE.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, s, p, o, training=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, s,p,o, training=False):
    
    phase_s = tf.math.atan2(tf.math.sin(s),tf.math.cos(s))
    phase_p = tf.math.atan2(tf.math.sin(p),tf.math.cos(p))
    phase_o = tf.math.atan2(tf.math.sin(o),tf.math.cos(o))
    
    score = tf.abs(tf.math.sin((phase_s + phase_p - phase_o)/2))
    if self.gamma &gt; 0:
        return self.gamma - tf.reduce_sum(score,axis=1)*self.modulus
    else:
        return tf.reduce_sum(score,axis=1)*self.modulus</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></b></code>:
<ul class="hlist">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="KGEkeras" href="index.html">KGEkeras</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="KGEkeras.models.l3_reg" href="#KGEkeras.models.l3_reg">l3_reg</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="KGEkeras.models.ComplEx" href="#KGEkeras.models.ComplEx">ComplEx</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.ComplEx.func" href="#KGEkeras.models.ComplEx.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.ConvE" href="#KGEkeras.models.ConvE">ConvE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.ConvE.func" href="#KGEkeras.models.ConvE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.ConvKB" href="#KGEkeras.models.ConvKB">ConvKB</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.ConvKB.func" href="#KGEkeras.models.ConvKB.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.ConvR" href="#KGEkeras.models.ConvR">ConvR</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.ConvR.func" href="#KGEkeras.models.ConvR.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.CosinE" href="#KGEkeras.models.CosinE">CosinE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.CosinE.func" href="#KGEkeras.models.CosinE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.DistMult" href="#KGEkeras.models.DistMult">DistMult</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.DistMult.func" href="#KGEkeras.models.DistMult.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.EmbeddingModel" href="#KGEkeras.models.EmbeddingModel">EmbeddingModel</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.EmbeddingModel.call" href="#KGEkeras.models.EmbeddingModel.call">call</a></code></li>
<li><code><a title="KGEkeras.models.EmbeddingModel.get_config" href="#KGEkeras.models.EmbeddingModel.get_config">get_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.HAKE" href="#KGEkeras.models.HAKE">HAKE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.HAKE.func" href="#KGEkeras.models.HAKE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.HolE" href="#KGEkeras.models.HolE">HolE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.HolE.func" href="#KGEkeras.models.HolE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.ModE" href="#KGEkeras.models.ModE">ModE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.ModE.func" href="#KGEkeras.models.ModE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.RotatE" href="#KGEkeras.models.RotatE">RotatE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.RotatE.func" href="#KGEkeras.models.RotatE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.TransE" href="#KGEkeras.models.TransE">TransE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.TransE.func" href="#KGEkeras.models.TransE.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="KGEkeras.models.pRotatE" href="#KGEkeras.models.pRotatE">pRotatE</a></code></h4>
<ul class="">
<li><code><a title="KGEkeras.models.pRotatE.func" href="#KGEkeras.models.pRotatE.func">func</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>